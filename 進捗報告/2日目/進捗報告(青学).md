---
marp: true
theme: test
class:
  - slide
---

<!-- paginate: true -->

# 進捗報告(青学用)

---

## 目次

* 先週のモデルの成果
* 考察
* 模索したこと
* 物理の勉強の進捗

---

## 先週のモデルの成果

* LE-conformer(損失 __0.07__ 程度)はTransformer(損失 __0.1__ 程度)よりは良さそうな損失になっていたが、既存のフルコネクションモデル(損失 __0.04__ 程度)を超えるものではなかった。
* LE-conformerは、人が決めなければいけないパラメータや一部分だがアーキテクチャそのものも自分で決めなければいけないので、その部分のアーキテクチャを組み替えて色々やっていたのがこの一週間の主な内容
* 結果としては、埋め込み層はfc、出力層はflattenをした後にfc層が良さそうだった。(今東大の方で学習中だが、昨日覗いたら損失 __0.046__ くらいにはなっていた。ここからもう少し下がってくれるかは別問題だが。)


---

## 考察

* Transformer系全般に感じることだが、train損失が減るのがfcと比べてものすごく早い。学習が途中で行われなくなっていると思う。
* Transformer系のモデルがfcと比べて過度に複雑なモデルになっていて過剰にデータにフィットしてる可能性…？

---

## 模索したこと

* 私と同じようなタスクで機械学習をやっている前例はないかと思い、調べたら「データは世界に３つある重力波検出器の観測をシミュレートした長さ2秒、4096 のデータ３つに重力波が含まれているかどうかを判定する二値分類。」というタスクがGoogleがコンペという形でやっていた。
* このコンペではいい成績を残した2つのモデルがある。
    1. 信号データをスペクトログラム に変換してそれを画像とみなし画像用のモデルに突っ込むやり方。
    2. 1DCNNを工夫して精度を上げる方法(これがかなり難しく勝負の分かれ目になったらしい)

---

## 模索したこと

* ⅰのやり方はシーケンス長が長くて初めてできる技なので割愛(私のデータは101次元、コンペは4096次元)
* ⅱのやり方は1次元CNNで直接データを扱っていたので何か知見は得られそう。
* GeM Poolという学習可能なプーリング層を使っていて、初めて知った。活性化関数はSiLU関数(Swish)。1DCNNを6層(うち3層はGeMを使用)して最後はfc→fc→fcと噛ませて1次元まで落としていた。

---

## 物理の勉強の進捗

とりあえず以下の項目を学びました。(と言っても微分方程式を自分で解いて確かめて行ったわけではないですが。。)

* シュレディンガー方程式、固有関数、固有値
* 有限/無限井戸型ポテンシャル、立方体ポテンシャル(初めての縮退している例)、トンネル効果
* 調和振動子
* 中心力場ポテンシャル(=水素原子)

などです。とりあえず水素原子までいって、球面調和関数とか電子軌道の話(主量子数、方位両指数、磁気両指数)とかの話はわかりました。摂動論、ブラ・ケットベクトル、スピンあたりは来週までの目標。

